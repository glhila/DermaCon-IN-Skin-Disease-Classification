Index: train_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nimport time\nfrom data_preparation import prepare_data\n\n# צבעים להדגשה\nRED = '\\033[91m'\nYELLOW = '\\033[93m'\nENDC = '\\033[0m'\n\ndef train_model(num_epochs=10):\n    \"\"\"\n    Train MobileNetV2 with more unfrozen layers (mid + deep) for better fine-tuning.\n    \"\"\"\n\n    # Load DataLoaders\n    train_loader, val_loader, test_loader, label_encoder = prepare_data()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load pretrained MobileNetV2\n    mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n\n    # Freeze all layers first\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Replace the classifier head\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n\n    # Unfreeze **more layers** (mid + deep + classifier)\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\n            \"features.10\", \"features.11\", \"features.12\", \"features.13\",\n            \"features.14\", \"features.15\", \"features.16\", \"features.17\",\n            \"classifier\"\n        ]):\n            param.requires_grad = True\n\n    # Move to device\n    mobilenet = mobilenet.to(device)\n\n    # Check frozen status\n    frozen = sum([not param.requires_grad for param in mobilenet.parameters()])\n    total = len(list(mobilenet.parameters()))\n    print(f\"{YELLOW}✅ Frozen {frozen} out of {total} layers.{ENDC}\")\n    print(f\"{YELLOW}Note: Even fewer layers are frozen now — more layers are fine-tuning!{ENDC}\")\n\n    # Loss and optimizer (lower LR for sensitive fine-tuning)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=0.0002)\n\n    # Training loop\n    print(f\"\\n{YELLOW}\uD83D\uDE80 Starting training for {num_epochs} epochs with deeper fine-tuning...{ENDC}\")\n    start_time = time.time()\n\n    losses = []\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss = running_loss / total\n        epoch_acc = correct / total\n        losses.append(epoch_loss)\n\n        epoch_time = time.time() - epoch_start\n        est_total_time = epoch_time * num_epochs\n        est_remaining = est_total_time - epoch_time * (epoch + 1)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f} | Time: {epoch_time:.1f}s | Estimated time left: {est_remaining/60:.1f} min\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n{YELLOW}✅ Training complete. Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes).{ENDC}\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Model saved to mobilenetv2_binary.pth{ENDC}\")\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n\n    # Final loss trend summary\n    if len(losses) >= 2 and losses[-1] < losses[0]:\n        print(f\"\\n{YELLOW}\uD83D\uDCC9 Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f} — deeper fine-tuning improved training!{ENDC}\")\n    else:\n        print(f\"\\n{RED}⚠\uFE0F Loss did NOT decrease — consider even more epochs or different LR.{ENDC}\")\n\n    print(f\"\\n{RED}\uD83E\uDDEA You can now run evaluation on test_loader if desired!{ENDC}\")\n\n\ndef continue_training(num_epochs=10, lr=0.0002):\n    # Load DataLoaders\n    train_loader, val_loader, test_loader, label_encoder = prepare_data()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load the model you already trained\n    mobilenet = models.mobilenet_v2(weights=None)\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n    mobilenet.load_state_dict(torch.load(\"mobilenetv2_binary.pth\", map_location=device))\n    mobilenet = mobilenet.to(device)\n\n    # Freeze all layers first\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Keep the same layers unfrozen (features.10–17 + classifier)\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\n            \"features.10\", \"features.11\", \"features.12\", \"features.13\",\n            \"features.14\", \"features.15\", \"features.16\", \"features.17\",\n            \"classifier\"\n        ]):\n            param.requires_grad = True\n\n    # Check frozen status\n    frozen = sum([not p.requires_grad for p in mobilenet.parameters()])\n    total = len(list(mobilenet.parameters()))\n    print(f\"{YELLOW}✅ Continuing training with {total - frozen} layers trainable.{ENDC}\")\n\n    # Loss and optimizer (lower LR for fine-tuning)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=lr)\n\n    # Training loop\n    print(f\"\\n{YELLOW}\uD83D\uDE80 Continuing fine-tuning for {num_epochs} more epochs...{ENDC}\")\n    start_time = time.time()\n    losses = []\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total_samples += labels.size(0)\n\n        epoch_loss = running_loss / total_samples\n        epoch_acc = correct / total_samples\n        losses.append(epoch_loss)\n\n        epoch_time = time.time() - epoch_start\n        est_remaining = epoch_time * (num_epochs - epoch - 1)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f} | Time: {epoch_time:.1f}s | Estimated time left: {est_remaining/60:.1f} min\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n{YELLOW}✅ Fine-tuning complete. Total time: {total_time/60:.1f} minutes.{ENDC}\")\n\n    # Save updated model\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Updated model saved to mobilenetv2_binary.pth{ENDC}\")\n\n    # Loss trend\n    if len(losses) >= 2 and losses[-1] < losses[0]:\n        print(f\"\\n{YELLOW}\uD83D\uDCC9 Loss improved further from {losses[0]:.4f} → {losses[-1]:.4f}!{ENDC}\")\n    else:\n        print(f\"\\n{YELLOW}ℹ\uFE0F Loss trend stable, might need more layers or more epochs next.{ENDC}\")\n\n\n\ndef deep_finetune(num_epochs=10, lr=0.0001):\n    # Load DataLoaders\n    train_loader, val_loader, test_loader, label_encoder = prepare_data()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load the current trained model\n    mobilenet = models.mobilenet_v2(weights=None)\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n    mobilenet.load_state_dict(torch.load(\"mobilenetv2_binary.pth\", map_location=device))\n    mobilenet = mobilenet.to(device)\n\n    # Freeze all layers first\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Unfreeze MORE layers now: features.8–17 + classifier\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\n            \"features.8\", \"features.9\",\n            \"features.10\", \"features.11\", \"features.12\",\n            \"features.13\", \"features.14\", \"features.15\",\n            \"features.16\", \"features.17\",\n            \"classifier\"\n        ]):\n            param.requires_grad = True\n\n    # Check frozen vs trainable layers\n    frozen = sum([not p.requires_grad for p in mobilenet.parameters()])\n    total = len(list(mobilenet.parameters()))\n    print(f\"{YELLOW}✅ Deep fine-tuning with {total - frozen} layers trainable now.{ENDC}\")\n\n    # Loss and optimizer (low LR for safe fine-tuning)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=lr)\n\n    # Training loop\n    print(f\"\\n{YELLOW}\uD83D\uDE80 Deep fine-tuning for {num_epochs} more epochs...{ENDC}\")\n    start_time = time.time()\n    losses = []\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total_samples += labels.size(0)\n\n        epoch_loss = running_loss / total_samples\n        epoch_acc = correct / total_samples\n        losses.append(epoch_loss)\n\n        epoch_time = time.time() - epoch_start\n        est_remaining = epoch_time * (num_epochs - epoch - 1)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f} | Time: {epoch_time:.1f}s | Estimated time left: {est_remaining/60:.1f} min\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n{YELLOW}✅ Deep fine-tuning complete. Total time: {total_time/60:.1f} minutes.{ENDC}\")\n\n    # Save updated model\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Updated model saved to mobilenetv2_binary.pth{ENDC}\")\n\n    # Loss trend summary\n    if len(losses) >= 2 and losses[-1] < losses[0]:\n        print(f\"\\n{YELLOW}\uD83D\uDCC9 Loss improved further from {losses[0]:.4f} → {losses[-1]:.4f}!{ENDC}\")\n    else:\n        print(f\"\\n{YELLOW}ℹ\uFE0F Loss trend stable, next step could be even earlier layers or class weights.{ENDC}\")\n\n    print(f\"\\n{YELLOW}\uD83D\uDC49 After this finishes, run model_test.py again to check if Inflammatory improved!{ENDC}\")\n\n\ndef weighted_finetune(num_epochs=8, lr=0.0001, weight_factor=1.3):\n    # Load DataLoaders\n    train_loader, val_loader, test_loader, label_encoder = prepare_data()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load the current trained model\n    mobilenet = models.mobilenet_v2(weights=None)\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n    mobilenet.load_state_dict(torch.load(\"mobilenetv2_binary.pth\", map_location=device))\n    mobilenet = mobilenet.to(device)\n\n    # Freeze all layers first\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Unfreeze the same deeper layers (features.8–17 + classifier)\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\n            \"features.8\", \"features.9\",\n            \"features.10\", \"features.11\", \"features.12\",\n            \"features.13\", \"features.14\", \"features.15\",\n            \"features.16\", \"features.17\",\n            \"classifier\"\n        ]):\n            param.requires_grad = True\n\n    # Check frozen vs trainable layers\n    frozen = sum([not p.requires_grad for p in mobilenet.parameters()])\n    total = len(list(mobilenet.parameters()))\n    print(f\"{YELLOW}✅ Weighted fine-tuning with {total - frozen} layers trainable now.{ENDC}\")\n\n    # Class weights → give higher weight to \"Inflammatory\"\n    class_weights = torch.tensor([1.0, weight_factor]).to(device)\n    print(f\"{YELLOW}Using class weights: Infectious=1.0, Inflammatory={weight_factor}{ENDC}\")\n\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=lr)\n\n    # Training loop\n    print(f\"\\n{YELLOW}\uD83D\uDE80 Weighted fine-tuning for {num_epochs} epochs...{ENDC}\")\n    start_time = time.time()\n    losses = []\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total_samples += labels.size(0)\n\n        epoch_loss = running_loss / total_samples\n        epoch_acc = correct / total_samples\n        losses.append(epoch_loss)\n\n        epoch_time = time.time() - epoch_start\n        est_remaining = epoch_time * (num_epochs - epoch - 1)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f} | Time: {epoch_time:.1f}s | Estimated time left: {est_remaining/60:.1f} min\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n{YELLOW}✅ Weighted fine-tuning complete. Total time: {total_time/60:.1f} minutes.{ENDC}\")\n\n    # Save updated model\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Updated model saved to mobilenetv2_binary.pth{ENDC}\")\n\n    # Loss trend summary\n    if len(losses) >= 2 and losses[-1] < losses[0]:\n        print(f\"\\n{YELLOW}\uD83D\uDCC9 Loss improved further from {losses[0]:.4f} → {losses[-1]:.4f}!{ENDC}\")\n    else:\n        print(f\"\\n{YELLOW}ℹ\uFE0F Loss trend stable, next step could be even more class balance tuning or augmentations.{ENDC}\")\n\n    print(f\"\\n{YELLOW}\uD83D\uDC49 After this finishes, run model_test.py again to check if Inflammatory recall improved!{ENDC}\")\n\ndef finetune_feature18(num_epochs=5, lr=0.00005):\n    \"\"\"\n    Fine-tunes only the features.18 block and classifier of MobileNetV2.\n    Loads model from and saves back to 'mobilenetv2_binary.pth'.\n    \"\"\"\n\n    print(f\"{YELLOW}--- Starting Feature 18 Fine-tuning ---{ENDC}\")\n\n    # Load data\n    train_loader, val_loader, test_loader, _ = prepare_data()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load the pretrained model\n    mobilenet = models.mobilenet_v2(weights=None)\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n\n    try:\n        mobilenet.load_state_dict(torch.load(\"mobilenetv2_binary.pth\", map_location=device))\n        print(f\"{YELLOW}Loaded weights from mobilenetv2_binary.pth{ENDC}\")\n    except Exception as e:\n        print(f\"{RED}Error loading pretrained model: {e}{ENDC}\")\n        return\n\n    mobilenet = mobilenet.to(device)\n\n    # Freeze all layers\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Unfreeze only features.18 and classifier\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\"features.18\", \"classifier\"]):\n            param.requires_grad = True\n\n    # Check how many parameters are being trained\n    total_params = len(list(mobilenet.parameters()))\n    trainable_params = sum(p.requires_grad for p in mobilenet.parameters())\n    print(f\"{YELLOW}Fine-tuning only features.18 and classifier ({trainable_params} out of {total_params} parameters).{ENDC}\")\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=lr)\n\n    # Training loop\n    print(f\"{YELLOW}\uD83D\uDE80 Starting fine-tuning for {num_epochs} epochs...{ENDC}\")\n    start_time = time.time()\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            correct += outputs.argmax(dim=1).eq(labels).sum().item()\n            total_samples += labels.size(0)\n\n        avg_loss = running_loss / total_samples\n        acc = correct / total_samples\n        epoch_time = time.time() - epoch_start\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Accuracy: {acc:.4f} | Time: {epoch_time:.1f}s\")\n\n    total_time = time.time() - start_time\n    print(f\"{YELLOW}✅ Fine-tuning complete. Total time: {total_time/60:.1f} minutes.{ENDC}\")\n\n    # Save updated model back to same file\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Updated model saved to mobilenetv2_binary.pth (overwritten){ENDC}\")\n\ndef quantize():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # === 1. Load trained model ===\n    mobilenet = models.mobilenet_v2(weights=None)\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n\n    mobilenet.load_state_dict(torch.load(\"mobilenetv2_binary.pth\", map_location=device))\n    mobilenet = mobilenet.to(device)\n    mobilenet.eval()\n    print(\"✅ Original model loaded\")\n\n    # === 2. Apply dynamic quantization ===\n    quantized_model = torch.quantization.quantize_dynamic(\n        mobilenet, {nn.Linear}, dtype=torch.qint8\n    )\n    torch.save(quantized_model.state_dict(), \"mobilenetv2_binary_quantized.pth\")\n    print(\"✅ Quantized model saved\")\n\n\nif __name__ == \"__main__\":\n    train_model(num_epochs=10)\n    continue_training(num_epochs=10, lr=0.0002)\n    deep_finetune(num_epochs=10, lr=0.0001)\n\n    # You can tweak weight_factor (1.2–1.5) if needed\n    weighted_finetune(num_epochs=8, lr=0.0001, weight_factor=1.3)\n    finetune_feature18()\n    quantize()\n\n
===================================================================
diff --git a/train_model.py b/train_model.py
--- a/train_model.py	(revision dafe3018ba5a53675e29d7e574395cc7cb17ffbd)
+++ b/train_model.py	(date 1753982859678)
@@ -5,7 +5,7 @@
 import time
 from data_preparation import prepare_data
 
-# צבעים להדגשה
+# Define color codes for terminal output
 RED = '\033[91m'
 YELLOW = '\033[93m'
 ENDC = '\033[0m'
@@ -17,31 +17,39 @@
 
     # Load DataLoaders
     train_loader, val_loader, test_loader, label_encoder = prepare_data()
-
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
     print(f"{YELLOW}Using device: {device}{ENDC}")
 
-    # Load pretrained MobileNetV2
+    # Load model MobileNetV2
     mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
+    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)
+    try:
+        mobilenet.load_state_dict(torch.load("mobilenetv2_binary.pth", map_location=device))
+        print(f"{YELLOW}Loaded weights from mobilenetv2_binary.pth{ENDC}")
+    except FileNotFoundError:
+        print(f"{RED}Error: mobilenetv2_binary.pth not found! Starting from scratch.{ENDC}")
+        # If the file doesn't exist, we will train from the pretrained weights
+        mobilenet.load_state_dict(mobilenet.state_dict())  # Load default weights
+    mobilenet = mobilenet.to(device)
 
     # Freeze all layers first
     for param in mobilenet.parameters():
         param.requires_grad = False
 
     # Replace the classifier head
-    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)
 
-    # Unfreeze **more layers** (mid + deep + classifier)
+
+    # Unfreeze required layers (features.8–18 + classifier)
     for name, param in mobilenet.named_parameters():
         if any(layer in name for layer in [
-            "features.10", "features.11", "features.12", "features.13",
-            "features.14", "features.15", "features.16", "features.17",
+            "features.8", "features.9","features.10", "features.11", "features.12",
+            "features.13", "features.14", "features.15","features.16", "features.17",
             "classifier"
         ]):
             param.requires_grad = True
 
     # Move to device
-    mobilenet = mobilenet.to(device)
+
 
     # Check frozen status
     frozen = sum([not param.requires_grad for param in mobilenet.parameters()])
@@ -206,13 +214,13 @@
     for param in mobilenet.parameters():
         param.requires_grad = False
 
-    # Unfreeze MORE layers now: features.8–17 + classifier
+    # Unfreeze MORE layers now: features.8–18 + classifier
     for name, param in mobilenet.named_parameters():
         if any(layer in name for layer in [
             "features.8", "features.9",
             "features.10", "features.11", "features.12",
             "features.13", "features.14", "features.15",
-            "features.16", "features.17",
+            "features.16", "features.17","features.18",
             "classifier"
         ]):
             param.requires_grad = True
Index: model_training.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nimport time\nfrom data_preparation import prepare_data\n\n# צבעים להדגשה\nYELLOW = '\\033[93m'\nRED = '\\033[91m'\nENDC = '\\033[0m'\n\ndef train_model(num_epochs=10):\n    \"\"\"\n    Train MobileNetV2 with more unfrozen layers (mid + deep) for better fine-tuning.\n    \"\"\"\n\n    # Load DataLoaders\n    train_loader, val_loader, test_loader, label_encoder = prepare_data()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{YELLOW}Using device: {device}{ENDC}\")\n\n    # Load pretrained MobileNetV2\n    mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n\n    # Freeze all layers first\n    for param in mobilenet.parameters():\n        param.requires_grad = False\n\n    # Replace the classifier head\n    mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 2)\n\n    # Unfreeze **more layers** (mid + deep + classifier)\n    for name, param in mobilenet.named_parameters():\n        if any(layer in name for layer in [\n            \"features.10\", \"features.11\", \"features.12\", \"features.13\",\n            \"features.14\", \"features.15\", \"features.16\", \"features.17\",\n            \"classifier\"\n        ]):\n            param.requires_grad = True\n\n    # Move to device\n    mobilenet = mobilenet.to(device)\n\n    # Check frozen status\n    frozen = sum([not param.requires_grad for param in mobilenet.parameters()])\n    total = len(list(mobilenet.parameters()))\n    print(f\"{YELLOW}✅ Frozen {frozen} out of {total} layers.{ENDC}\")\n    print(f\"{YELLOW}Note: Even fewer layers are frozen now — more layers are fine-tuning!{ENDC}\")\n\n    # Loss and optimizer (lower LR for sensitive fine-tuning)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, mobilenet.parameters()), lr=0.0002)\n\n    # Training loop\n    print(f\"\\n{YELLOW}\uD83D\uDE80 Starting training for {num_epochs} epochs with deeper fine-tuning...{ENDC}\")\n    start_time = time.time()\n\n    losses = []\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        mobilenet.train()\n\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = mobilenet(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\n        epoch_loss = running_loss / total\n        epoch_acc = correct / total\n        losses.append(epoch_loss)\n\n        epoch_time = time.time() - epoch_start\n        est_total_time = epoch_time * num_epochs\n        est_remaining = est_total_time - epoch_time * (epoch + 1)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f} | Time: {epoch_time:.1f}s | Estimated time left: {est_remaining/60:.1f} min\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n{YELLOW}✅ Training complete. Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes).{ENDC}\")\n    print(f\"{YELLOW}\uD83D\uDCE6 Model saved to mobilenetv2_binary.pth{ENDC}\")\n    torch.save(mobilenet.state_dict(), \"mobilenetv2_binary.pth\")\n\n    # Final loss trend summary\n    if len(losses) >= 2 and losses[-1] < losses[0]:\n        print(f\"\\n{YELLOW}\uD83D\uDCC9 Loss decreased from {losses[0]:.4f} to {losses[-1]:.4f} — deeper fine-tuning improved training!{ENDC}\")\n    else:\n        print(f\"\\n{RED}⚠\uFE0F Loss did NOT decrease — consider even more epochs or different LR.{ENDC}\")\n\n    print(f\"\\n{RED}\uD83E\uDDEA You can now run evaluation on test_loader if desired!{ENDC}\")\n\n\nif __name__ == \"__main__\":\n    train_model(num_epochs=10)\n
===================================================================
diff --git a/model_training.py b/model_training.py
--- a/model_training.py	(revision dafe3018ba5a53675e29d7e574395cc7cb17ffbd)
+++ b/model_training.py	(date 1753876842461)
@@ -5,7 +5,7 @@
 import time
 from data_preparation import prepare_data
 
-# צבעים להדגשה
+# Define color codes for terminal output
 YELLOW = '\033[93m'
 RED = '\033[91m'
 ENDC = '\033[0m'
